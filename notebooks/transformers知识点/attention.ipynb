{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 为什么是要除以一个d_k(scaled)?\n",
    "- 本质是为了稳定梯度流\n",
    "- 因为softmax在"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 首先对于两个独立的标准正太分布q, k 它的均值为0, 方差为1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "均值 tensor(-0.0349)\n",
      "方差 tensor(1.0463)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "q  = torch.randn(1, 5, 200)\n",
    "k = torch.randn(1, 5, 200)\n",
    "print('均值', torch.mean(q))\n",
    "print('方差', torch.var(q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 点积之后的方差会变大"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(158.0351)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = torch.matmul(q, k.transpose(1, 2))\n",
    "logits.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7613.2290)"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(q, q.transpose(1, 2)).var()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 除以 $\\sqrt{d_k}$ 方差就会变小, 进而不会出现非常大的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled = logits / torch.sqrt(torch.tensor(200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7902)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled.var()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 因为softmax在输入数量级比较大的时候会把全部概率分配给最大的值\n",
    "- 假设输入为$x = [a, a, 2a]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.linspace(0, 100, 100)\n",
    "f = lambda x: torch.exp(x * 2) / (torch.exp(x) + torch.exp(x) + torch.exp(x * 2))\n",
    "y = [f(x_i) for x_i in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f5ee39580d0>]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x, y)   # 可以看到softmax将所有概率给了2a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 也就是说softmax在输入数量级比较大的时候, 梯度会很小(消失)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "#对比实验\n",
    "class MulAttention(torch.nn.Module):\n",
    "    def __init__(self, scale):\n",
    "        super().__init__()\n",
    "        self.scale =scale\n",
    "    def forward(self, q, k, v):\n",
    "        logits = torch.matmul(q, k.transpose(-2, -1)) / self.scale\n",
    "        scores = F.softmax(logits, dim=-1)\n",
    "        output = torch.matmul(scores, v)\n",
    "        return output\n",
    "        \n",
    "\n",
    "class AttentionModel(torch.nn.Module):\n",
    "    def __init__(self, dim, scale):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.scale = torch.tensor(scale)\n",
    "        self.attentions = torch.nn.ModuleList([MulAttention(self.scale) for _ in range(12)])\n",
    "        self.classifier = torch.nn.Linear(dim, 10)\n",
    "\n",
    "    def forward(self, x, q, k, v):\n",
    "        for attn in self.attentions:\n",
    "            x = attn(x)\n",
    "        print(x.shape)\n",
    "        logits = self.classifier(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = torch.randn(10, 100, requires_grad=True)\n",
    "x2 = torch.randn(10, 10000, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AttentionModel(dim=100, scale=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 100])\n"
     ]
    }
   ],
   "source": [
    "y = model(x1)\n",
    "loss = torch.nn.CrossEntropyLoss()(y, torch.tensor([0,1,2,3,3,4,4,5,2,4]))\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(0.0023),\n",
       " tensor(0.0023),\n",
       " tensor(0.0024),\n",
       " tensor(0.0020),\n",
       " tensor(0.0023),\n",
       " tensor(0.0023),\n",
       " tensor(0.0025),\n",
       " tensor(0.0022),\n",
       " tensor(0.0024),\n",
       " tensor(0.0026)]"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[torch.max(grad) for grad in x2.grad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AttentionModel(dim=10000, scale=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 10000])\n"
     ]
    }
   ],
   "source": [
    "y = model(x2)\n",
    "loss = torch.nn.CrossEntropyLoss()(y, torch.tensor([0,1,2,3,3,4,4,5,2,4]))\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(0.0044),\n",
       " tensor(0.0054),\n",
       " tensor(0.0041),\n",
       " tensor(0.0037),\n",
       " tensor(0.0043),\n",
       " tensor(0.0043),\n",
       " tensor(0.0041),\n",
       " tensor(0.0043),\n",
       " tensor(0.0046),\n",
       " tensor(0.0046)]"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[torch.max(torch.abs(grad)) for grad in x2.grad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.16563217565858068, 0.026832742169559998, 0.03760569444895312, 0.24645576721826684, 0.24687804603104316]\n",
      "[0.00011828531697377631, 0.1822529814293652, 5.457536644826178e-10, 1.3216094885137863e-12, 9.595268468665097e-09]\n",
      "[0.07254351566421828, 0.08783516871558304, 0.13096732398840355, 0.11687844761880893, 0.07965179714885315]\n",
      "[0.1135116952703806, 0.13463629554899875, 0.23014945460707764, 0.09522641492313243, 0.07154209877134614]\n"
     ]
    }
   ],
   "source": [
    "from scipy.special import softmax\n",
    "import numpy as np\n",
    "\n",
    "def test_gradient(dim, time_steps=50, scale=1.0):\n",
    "    # Assume components of the query and keys are drawn from N(0, 1) independently\n",
    "    q = np.random.randn(dim)\n",
    "    ks = np.random.randn(time_steps, dim)\n",
    "    x = np.sum(q * ks, axis=1) / scale  # x.shape = (time_steps,) \n",
    "    y = softmax(x)\n",
    "    grad = np.diag(y) - np.outer(y, y)\n",
    "    return np.max(np.abs(grad))  # the maximum component of gradients\n",
    "\n",
    "NUMBER_OF_EXPERIMENTS = 5\n",
    "# results of 5 random runs without scaling\n",
    "print([test_gradient(100) for _ in range(NUMBER_OF_EXPERIMENTS)])\n",
    "print([test_gradient(1000) for _ in range(NUMBER_OF_EXPERIMENTS)])\n",
    "\n",
    "# results of 5 random runs with scaling\n",
    "print([test_gradient(100, scale=np.sqrt(100)) for _ in range(NUMBER_OF_EXPERIMENTS)])\n",
    "print([test_gradient(1000, scale=np.sqrt(1000)) for _ in range(NUMBER_OF_EXPERIMENTS)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dcbdf8897def68fcf57029bac2d67dba4d31d00a95ecaaebbb0a8aa84392f456"
  },
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
