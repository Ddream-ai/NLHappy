{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, list_datasets, list_metrics, load_from_disk, Dataset\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentence_a', 'sentence_b', 'label_id'],\n",
       "    num_rows: 247568\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = Dataset.from_pandas(df)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence_a', 'sentence_b', 'label_id'],\n",
       "        num_rows: 198054\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence_a', 'sentence_b', 'label_id'],\n",
       "        num_rows: 49514\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(e):\n",
    "    inputs = tokenizer(e['sentence_a'][0], max_length=50, truncation=True, padding='max_length')\n",
    "    inputs = dict(zip(inputs.keys(), map(torch.tensor, inputs.values())))\n",
    "    label = e['label_id'][0]\n",
    "    return {'inputs':[inputs], 'label':[label]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.set_transform(encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'inputs': {'input_ids': tensor([ 101, 6369, 5050, 3322, 1724, 5277,  680, 6369, 5050, 3322, 5440, 4777,\n",
      "        1525,  702, 7410, 8043,  102,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])}, 'label': 0}\n"
     ]
    }
   ],
   "source": [
    "print(ds['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(ds['train'], batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = next(iter(loader))['inputs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bert = BertModel.from_pretrained('bert-base-chinese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = bert(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 4.8468e-01, -1.5569e-01, -1.7471e-01,  ...,  1.6859e-01,\n",
       "          -5.5856e-01,  3.8753e-01],\n",
       "         [ 4.4400e-01,  5.8182e-01,  1.2014e+00,  ..., -5.0518e-02,\n",
       "          -6.3436e-01, -2.6499e-02],\n",
       "         [ 6.0337e-01, -1.6424e-01,  8.4813e-02,  ...,  2.6409e-01,\n",
       "           2.1916e-01, -1.2433e-01],\n",
       "         ...,\n",
       "         [ 7.6617e-01,  2.4411e-02, -4.8762e-01,  ...,  7.2499e-01,\n",
       "          -3.7644e-02,  4.8437e-02],\n",
       "         [ 6.4181e-01,  1.0708e-01, -6.2491e-01,  ...,  6.9181e-01,\n",
       "          -2.4639e-01,  1.4376e-01],\n",
       "         [ 2.0560e-01,  4.8218e-01, -8.6308e-01,  ...,  9.8600e-03,\n",
       "          -1.0057e-01,  1.1442e-01]],\n",
       "\n",
       "        [[-1.2120e-03,  3.7300e-01,  4.8251e-02,  ..., -7.1353e-02,\n",
       "           3.9020e-02, -1.9349e-01],\n",
       "         [ 6.2819e-01,  2.9261e-01,  8.9257e-01,  ..., -7.8841e-01,\n",
       "          -6.4216e-01,  1.4592e-01],\n",
       "         [ 6.7330e-01,  3.3981e-01,  1.4284e+00,  ..., -1.9897e-01,\n",
       "           4.5122e-01, -1.8851e-01],\n",
       "         ...,\n",
       "         [ 4.3610e-01,  1.3861e-01, -1.2555e-01,  ..., -1.2233e-01,\n",
       "          -2.2387e-01, -2.8153e-01],\n",
       "         [-1.7942e-01, -1.1125e-01, -3.8755e-01,  ...,  9.0128e-02,\n",
       "          -6.7991e-01, -4.9979e-02],\n",
       "         [-2.0873e-02, -1.0884e-01, -4.2074e-01,  ...,  1.5550e-01,\n",
       "          -6.4270e-01, -2.0721e-01]]], grad_fn=<NativeLayerNormBackward>), pooler_output=tensor([[ 0.9996,  0.9996,  1.0000,  ..., -0.9991, -0.9991,  0.5969],\n",
       "        [ 0.9999,  1.0000,  0.9999,  ..., -0.9935, -0.9998,  0.9400]],\n",
       "       grad_fn=<TanhBackward>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0ddff2849aab375ae0fd4b99e8a85b862ae377db9ed7ee27c4bfd51889afc517"
  },
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
